
@inproceedings{li_genetic_2022,
	address = {Zhuhai, China},
	title = {Genetic algorithm based hyper-parameters optimization for transfer convolutional neural network},
	isbn = {978-1-5106-5575-1 978-1-5106-5576-8},
	url = {https://www.spiedigitallibrary.org/conference-proceedings-of-spie/12285/2637170/Genetic-algorithm-based-hyper-parameters-optimization-for-transfer-convolutional-neural/10.1117/12.2637170.full},
	doi = {10.1117/12.2637170},
	abstract = {Hyperparameter optimization is a challenging problem in developing deep neural networks. Decision of transfer layers and trainable layers is a major task for design of the transfer convolutional neural networks (CNN). Conventional transfer CNN models are usually manually designed based on intuition. In this paper, a genetic algorithm is applied to select trainable layers of the transfer model. The filter criterion is constructed by accuracy and the counts of the trainable layers. The results show that the method is competent in this task. The system will converge with a precision of 97\% in the classification of Cats and Dogs datasets, in no more than 15 generations. Moreover, backward inference according the results of the genetic algorithm shows that our method can capture the gradient features in network layers, which plays a part on understanding of the transfer AI models.},
	language = {en},
	urldate = {2024-07-16},
	booktitle = {International {Conference} on {Advanced} {Algorithms} and {Neural} {Networks} ({AANN} 2022)},
	publisher = {SPIE},
	author = {Li, Chen and Jiang, Jinzhe and Zhao, Yaqian and Li, Rengang and Wang, Endong and Zhang, Xin and Zhao, Kun},
	editor = {Tiwari, Rajeev},
	month = jun,
	year = {2022},
	pages = {46},
	file = {Li et al. - 2022 - Genetic algorithm based hyper-parameters optimizat.pdf:/Users/jonasmetzger/Zotero/storage/4TCJ986C/Li et al. - 2022 - Genetic algorithm based hyper-parameters optimizat.pdf:application/pdf},
}

@article{chai_optimizing_2024,
	title = {Optimizing neural network training with {Genetic} {Algorithms}},
	volume = {42},
	issn = {2755-2721, 2755-273X},
	url = {https://ace.ewapublishing.org/article/e18d8a528a9046778d00a52d0676ed9c},
	doi = {10.54254/2755-2721/42/20230780},
	abstract = {In modern society, computer plays an important role among all human beings. Through the increasing development of technology, some problems happened gradually. In order to solve and regenerate the country, individuals should test their strengths. This paper discusses how to use genetic algorithms to optimize neural network training. As an important tool of machine learning, neural networks have made remarkable achievements in dealing with complex tasks. However, the training process of neural networks involves a lot of hyperparameter adjustment and weight optimization, which often requires a lot of time and computing resources. In order to improve the efficiency and performance of neural network training, humans should introduce genetic algorithms as an optimization method. Experiments are conducted on several common datasets to compare the performance of neural network training with Genetic Algorithm optimization against the traditional method. The results indicate that using Genetic Algorithms significantly improves the convergence speed and performance of neural networks while reducing the time and effort spent on hyperparameter tuning. Neural networks optimized using the Genetic Algorithm outperform their counterparts trained under the same time frame.},
	language = {en},
	number = {1},
	urldate = {2024-07-16},
	journal = {Applied and Computational Engineering},
	author = {Chai, Junen},
	month = feb,
	year = {2024},
	pages = {220--224},
	file = {Chai - 2024 - Optimizing neural network training with Genetic Al.pdf:/Users/jonasmetzger/Zotero/storage/5WR4L6LV/Chai - 2024 - Optimizing neural network training with Genetic Al.pdf:application/pdf},
}

@article{gomez-vargas_neural_2023,
	title = {Neural {Networks} {Optimized} by {Genetic} {Algorithms} in {Cosmology}},
	volume = {107},
	issn = {2470-0010, 2470-0029},
	url = {http://arxiv.org/abs/2209.02685},
	doi = {10.1103/PhysRevD.107.043509},
	abstract = {The applications of artiﬁcial neural networks in the cosmological ﬁeld have shone successfully during the past decade, this is due to their great ability of modeling large amounts of datasets and complex nonlinear functions. However, in some cases, their use still remains controversial because their ease of producing inaccurate results when the hyperparameters are not carefully selected. In this paper, to ﬁnd the optimal combination of hyperparameters to artiﬁcial neural networks, we propose to take advantage of the genetic algorithms. As a proof of the concept, we analyze three different cosmological cases to test the performance of the architectures achieved with the genetic algorithms and compare them with the standard process, consisting of a grid with all possible conﬁgurations. First, we carry out a model-independent reconstruction of the distance modulus using a type Ia supernovae compilation. Second, the neural networks learn to infer the equation of state for the quintessence model, and ﬁnally with the data from a combined redshift catalog the neural networks predict the photometric redshift given six photometric bands (urgizy). We found that the genetic algorithms improve considerably the generation of the neural network architectures, which can ensure more conﬁdence in their physical results because of the better performance in the metrics with respect to the grid method.},
	language = {en},
	number = {4},
	urldate = {2024-07-16},
	journal = {Physical Review D},
	author = {Gómez-Vargas, Isidro and Andrade, Joshua Briones and Vázquez, J. Alberto},
	month = feb,
	year = {2023},
	note = {arXiv:2209.02685 [astro-ph, stat]},
	keywords = {Astrophysics - Cosmology and Nongalactic Astrophysics, Astrophysics - Instrumentation and Methods for Astrophysics, Statistics - Machine Learning},
	pages = {043509},
	annote = {Comment: 15 pages, 4 figures; matches the version published in Physical Review D},
	file = {Gómez-Vargas et al. - 2023 - Neural Networks Optimized by Genetic Algorithms in.pdf:/Users/jonasmetzger/Zotero/storage/DTJHNUGQ/Gómez-Vargas et al. - 2023 - Neural Networks Optimized by Genetic Algorithms in.pdf:application/pdf},
}

@article{karlupia_genetic_2023,
	title = {A genetic algorithm based optimized convolutional neural network for face recognition},
	volume = {33},
	issn = {1641-876X, 2083-8492},
	url = {https://sciendo.com/article/10.34768/amcs-2023-0002},
	doi = {10.34768/amcs-2023-0002},
	language = {en},
	number = {1},
	urldate = {2024-07-16},
	journal = {International Journal of Applied Mathematics and Computer Science},
	author = {Karlupia, Namrata and Mahajan, Palak and Abrol, Pawanesh and Lehana, Parveen K.},
	year = {2023},
	file = {Full Text:/Users/jonasmetzger/Zotero/storage/DQ9XPPZJ/Karlupia et al. - 2023 - A genetic algorithm based optimized convolutional .pdf:application/pdf},
}

@misc{noauthor_kunstliches_2024,
	title = {Künstliches neuronales {Netz}},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://de.wikipedia.org/w/index.php?title=K%C3%BCnstliches_neuronales_Netz&oldid=245629039},
	abstract = {Künstliche neuronale Netze, auch künstliche neuronale Netzwerke, kurz: KNN (englisch artificial neural network, ANN), sind Netze aus künstlichen Neuronen, die von den Netzwerken inspiriert wurden, die biologische Neuronen in Gehirnen bilden. Ein KNN wird von künstlichen Neuronen gebildet, die miteinander verbunden sind und in der Regel in Schichten organisiert werden.
KNN werden beim Maschinellen Lernen eingesetzt. Damit können Computer Probleme lösen, die zu kompliziert sind, um sie mit Regeln zu beschreiben, zu denen es aber viele Daten gibt, die als Beispiele für die gewünschte Lösung dienen können.
KNN bilden die Basis für Deep Learning, das ab 2006 erhebliche Fortschritte bei der Analyse von großen Datenmengen erlaubte. Erfolgreiche Anwendungen des Deep Learning sind beispielsweise Bilderkennung und Spracherkennung.
KNNs sind Forschungsgegenstand sowohl des Maschinellen Lernens, welches ein Teilbereich der künstlichen Intelligenz ist, als auch der interdisziplinären Neuroinformatik. Das Nachbilden eines biologischen neuronalen Netzes von Neuronen ist eher Gegenstand der Computational Neuroscience.},
	language = {de},
	urldate = {2024-07-25},
	journal = {Wikipedia},
	month = jun,
	year = {2024},
	note = {Page Version ID: 245629039},
	file = {Snapshot:/Users/jonasmetzger/Zotero/storage/UG34ADTD/Künstliches_neuronales_Netz.html:text/html},
}

@misc{noauthor_evolutionarer_2024,
	title = {Evolutionärer {Algorithmus}},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://de.wikipedia.org/w/index.php?title=Evolution%C3%A4rer_Algorithmus&oldid=246435107},
	abstract = {Evolutionäre Algorithmen (EA) sind eine Klasse von stochastischen, metaheuristischen Optimierungsverfahren, deren Funktionsweise von der Evolution natürlicher Lebewesen inspiriert ist.
In Anlehnung an die Natur werden Lösungskandidaten für ein bestimmtes Problem künstlich evolviert, EA sind also naturanaloge Optimierungsverfahren. Die Zuordnung zu den stochastischen und metaheuristischen Algorithmen bedeutet vor allem, dass EA meist nicht die beste Lösung für ein Problem finden, aber bei Erfolg eine hinreichend gute, was in der Praxis vor allem bei NP-vollständigen Problemen bereits wünschenswert ist. Die Verfahren verschiedener EA unterscheiden sich untereinander in erster Linie durch die genutzten Selektions-, Rekombinations- und Mutationsoperatoren, das Genotyp-Phänotyp-Mapping sowie die Problemrepräsentation.
Die ersten praktischen Implementierungen evolutionärer Algorithmen wurden Ende der 1950er Jahre veröffentlicht, allerdings äußerten sich bereits in den vorhergehenden Jahrzehnten Wissenschaftler zum Potenzial der Evolution für maschinelles Lernen.
Es gibt vier Hauptströmungen, deren Konzepte zumindest historisch voneinander zu unterscheiden sind:

genetische Algorithmen
Evolutionsstrategien
genetische Programmierung und
evolutionäre Programmierung
Heute verschwimmen diese Abgrenzungen zunehmend. Für eine bestimmte Anwendung wird ein EA geeignet entworfen, wobei in den letzten Jahrzehnten viele verschiedene Algorithmen und einzelne Operatoren entwickelt wurden, die heute benutzt werden können.
Die Anwendungen von EA gehen über Optimierung und Suche hinaus und finden sich auch in Kunst, Modellierung und Simulation, insbesondere auch bei der Untersuchung evolutionsbiologischer Fragestellungen.},
	language = {de},
	urldate = {2024-07-25},
	journal = {Wikipedia},
	month = jul,
	year = {2024},
	note = {Page Version ID: 246435107},
	file = {Snapshot:/Users/jonasmetzger/Zotero/storage/S8LVKXIS/Evolutionärer_Algorithmus.html:text/html},
}

@article{university_malaysia_of_computer_science_and_engineering_crossover_2017,
	title = {Crossover and {Mutation} {Operators} of {Genetic} {Algorithms}},
	volume = {7},
	issn = {20103700},
	url = {http://www.ijmlc.org/index.php?m=content&c=index&a=show&catid=69&id=704},
	doi = {10.18178/ijmlc.2017.7.1.611},
	abstract = {Genetic algorithms (GA) are stimulated by population genetics and evolution at the population level where crossover and mutation comes from random variables. The problems of slow and premature convergence to suboptimal solution remain an existing struggle that GA is facing. Due to lower diversity in a population, it becomes challenging to locally exploit the solutions. In order to resolve these issues, the focus is now on reaching equilibrium between the explorative and exploitative features of GA. Therefore, the search process can be prompted to produce suitable GA solutions. This paper begins with an introduction, Section 2 describes the GA exploration and exploitation strategies to locate the optimum solutions. Section 3 and 4 present the lists of some prevalent mutation and crossover operators. This paper concludes that the key issue in developing a GA is to deliver a balance between explorative and exploitative features that complies with the combination of operators in order to produce exceptional performance as a GA as a whole.},
	language = {en},
	number = {1},
	urldate = {2024-07-25},
	journal = {International Journal of Machine Learning and Computing},
	author = {{University Malaysia of Computer Science and Engineering} and Lim, Siew Mooi and Sultan, Abu Bakar Md. and Sulaiman, Md. Nasir and Mustapha, Aida and Leong, K. Y.},
	month = feb,
	year = {2017},
	pages = {9--12},
	file = {University Malaysia of Computer Science and Engineering et al. - 2017 - Crossover and Mutation Operators of Genetic Algori.pdf:/Users/jonasmetzger/Zotero/storage/AAMUB37I/University Malaysia of Computer Science and Engineering et al. - 2017 - Crossover and Mutation Operators of Genetic Algori.pdf:application/pdf},
}

@article{wolfangel_chatgpt_2023,
	address = {Hamburg},
	title = {{ChatGPT}: {Wie} nah sind wir an der {Superintelligenz}?},
	issn = {0044-2070},
	shorttitle = {{ChatGPT}},
	url = {https://www.zeit.de/digital/2023-04/chatgpt-kuenstliche-intelligenz-forschung},
	abstract = {Künstliche Intelligenz könnte die menschliche irgendwann überflügeln. Manche halten das für Science-Fiction. Einige Forscher sehen erste Anzeichen dafür schon in GPT-4.},
	language = {de-DE},
	urldate = {2024-07-25},
	journal = {Die Zeit},
	author = {Wolfangel, Eva},
	month = apr,
	year = {2023},
	keywords = {Chatbot, ChatGPT, Forschung, Intelligenz, künstliche Intelligenz, Singularität, Sprachmodell, Superintelligenz},
	file = {Snapshot:/Users/jonasmetzger/Zotero/storage/7JMK8D5K/chatgpt-kuenstliche-intelligenz-forschung.html:text/html},
}

@incollection{hutter_hyperparameter_2019,
	address = {Cham},
	title = {Hyperparameter {Optimization}},
	isbn = {978-3-030-05317-8},
	url = {http://link.springer.com/10.1007/978-3-030-05318-5_1},
	abstract = {Recent interest in complex and computationally expensive machine learning models with many hyperparameters, such as automated machine learning (AutoML) frameworks and deep neural networks, has resulted in a resurgence of research on hyperparameter optimization (HPO). In this chapter, we give an overview of the most prominent approaches for HPO. We ﬁrst discuss blackbox function optimization methods based on model-free methods and Bayesian optimization. Since the high computational demand of many modern machine learning applications renders pure blackbox optimization extremely costly, we next focus on modern multi-ﬁdelity methods that use (much) cheaper variants of the blackbox function to approximately assess the quality of hyperparameter settings. Lastly, we point to open problems and future research directions.},
	language = {en},
	urldate = {2024-07-25},
	booktitle = {Automated {Machine} {Learning}},
	publisher = {Springer International Publishing},
	author = {Feurer, Matthias and Hutter, Frank},
	editor = {Hutter, Frank and Kotthoff, Lars and Vanschoren, Joaquin},
	year = {2019},
	doi = {10.1007/978-3-030-05318-5_1},
	note = {Series Title: The Springer Series on Challenges in Machine Learning},
	pages = {3--33},
	annote = {and it was also established early that different hyperparameter configurations tend to work best for different datasets” (Feurer and Hutter, 2019, p. 2)
},
	file = {Feurer and Hutter - 2019 - Hyperparameter Optimization.pdf:/Users/jonasmetzger/Zotero/storage/X78CZ86G/Feurer and Hutter - 2019 - Hyperparameter Optimization.pdf:application/pdf},
}

@article{stelldinger_naturanaloge_2024,
	title = {Naturanaloge {Optimierungsverfahren} – {Genetische} {Algorithmen}},
	language = {de},
	journal = {Künstliche Intelligenz},
	author = {Stelldinger, Prof. Dr. Peer},
	year = {2024},
	file = {Stelldinger - 2024 - Naturanaloge Optimierungsverfahren – Genetische Al.pdf:/Users/jonasmetzger/Zotero/storage/XLQAADJX/Stelldinger - 2024 - Naturanaloge Optimierungsverfahren – Genetische Al.pdf:application/pdf},
}

@article{yeh_simplified_nodate,
	title = {Simplified {Swarm} {Optimisation} for the {Hyperparameters} of a {Convolutional} {Neural} {Network}},
	language = {en},
	author = {Yeh, Wei-Chang and Lin, Yi-Ping and Liang, Yun-Chia and Lai, Chyh-Ming and Gao, Xiao-Zhi},
	file = {Yeh et al. - Simplified Swarm Optimisation for the Hyperparamet.pdf:/Users/jonasmetzger/Zotero/storage/P2ZEBYMF/Yeh et al. - Simplified Swarm Optimisation for the Hyperparamet.pdf:application/pdf},
}

@misc{schaul_no_2013,
	title = {No {More} {Pesky} {Learning} {Rates}},
	url = {http://arxiv.org/abs/1206.1106},
	abstract = {The performance of stochastic gradient descent (SGD) depends critically on how learning rates are tuned and decreased over time. We propose a method to automatically adjust multiple learning rates so as to minimize the expected error at any one time. The method relies on local gradient variations across samples. In our approach, learning rates can increase as well as decrease, making it suitable for non-stationary problems. Using a number of convex and non-convex learning tasks, we show that the resulting algorithm matches the performance of SGD or other adaptive approaches with their best settings obtained through systematic search, and eﬀectively removes the need for learning rate tuning.},
	language = {en},
	urldate = {2024-07-27},
	publisher = {arXiv},
	author = {Schaul, Tom and Zhang, Sixin and LeCun, Yann},
	month = feb,
	year = {2013},
	note = {arXiv:1206.1106 [cs, stat]},
	keywords = {Statistics - Machine Learning, Computer Science - Machine Learning},
	file = {Schaul et al. - 2013 - No More Pesky Learning Rates.pdf:/Users/jonasmetzger/Zotero/storage/S7GIQJJP/Schaul et al. - 2013 - No More Pesky Learning Rates.pdf:application/pdf},
}

@book{goodfellow_deep_2016,
	address = {Cambridge, Massachusetts},
	series = {Adaptive computation and machine learning},
	title = {Deep learning},
	isbn = {978-0-262-03561-3},
	publisher = {The MIT Press},
	author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
	year = {2016},
	keywords = {Machine learning},
	annote = {Applied math and machine learning basics. Linear algebra -- Probability and information theory -- Numerical computation -- Machine learning basics -- Deep networks: modern practices. Deep feedforward networks -- Regularization for deep learning -- Optimization for training deep models -- Convolutional networks -- Sequence modeling: recurrent and recursive nets -- Practical methodology -- Applications -- Deep learning research. Linear factor models -- Autoencoders -- Representation learning -- Structured probabilistic models for deep learning -- Monte Carlo methods -- Confronting the partition function -- Approximate inference -- Deep generative models},
	file = {Ian Goodfellow, Yoshua Bengio, Aaron Courville - Deep Learning (2017, MIT).pdf:/Users/jonasmetzger/Zotero/storage/PTA3Y72X/Ian Goodfellow, Yoshua Bengio, Aaron Courville - Deep Learning (2017, MIT).pdf:application/pdf},
}

@article{kruse_evolutionare_2013,
	title = {Evolutionäre {Algorithmen} {Variation} und genetische {Operatoren}},
	url = {http://fuzzy.cs.ovgu.de/ci/ea/ea2013_v04_operatoren.pdf},
	language = {de},
	author = {Kruse, R and Held, P},
	month = apr,
	year = {2013},
	file = {Kruse and Held - Otto-von-Guericke-Universität Magdeburg Fakultät f.pdf:/Users/jonasmetzger/Zotero/storage/W2RCJ95G/Kruse and Held - Otto-von-Guericke-Universität Magdeburg Fakultät f.pdf:application/pdf},
}

@article{herrera_tackling_1998,
	title = {Tackling {Real}-{Coded} {Genetic} {Algorithms}: {Operators} and {Tools} for {Behavioural} {Analysis}},
	abstract = {Genetic algorithms play a signiﬁcant role, as search techniques for handling complex spaces, in many ﬁelds such as artiﬁcial intelligence, engineering, robotic, etc. Genetic algorithms are based on the underlying genetic process in biological organisms and on the natural evolution principles of populations. These algorithms process a population of chromosomes, which represent search space solutions, with three operations: selection, crossover and mutation.},
	language = {en},
	author = {Herrera, F and Lozano, M and Verdegay, J L},
	year = {1998},
	file = {Herrera et al. - Tackling Real-Coded Genetic Algorithms Operators .pdf:/Users/jonasmetzger/Zotero/storage/2P6KQU4D/Herrera et al. - Tackling Real-Coded Genetic Algorithms Operators .pdf:application/pdf},
}

@misc{noauthor_mnist_nodate,
	title = {{MNIST} handwritten digit database, {Yann} {LeCun}, {Corinna} {Cortes} and {Chris} {Burges}},
	url = {https://yann.lecun.com/exdb/mnist/},
	urldate = {2024-07-29},
	file = {MNIST handwritten digit database, Yann LeCun, Corinna Cortes and Chris Burges:/Users/jonasmetzger/Zotero/storage/7ZNRDWZE/mnist.html:text/html},
}

@book{sonnet_neuronale_2022,
	address = {Wiesbaden},
	series = {{IT} kompakt},
	title = {Neuronale {Netze} kompakt: {Vom} {Perceptron} zum {Deep} {Learning}},
	copyright = {https://www.springer.com/tdm},
	isbn = {978-3-658-29080-1},
	shorttitle = {Neuronale {Netze} kompakt},
	url = {https://link.springer.com/10.1007/978-3-658-29081-8},
	language = {de},
	urldate = {2024-07-30},
	publisher = {Springer Fachmedien Wiesbaden},
	author = {Sonnet, Daniel},
	year = {2022},
	doi = {10.1007/978-3-658-29081-8},
	file = {Sonnet - 2022 - Neuronale Netze kompakt Vom Perceptron zum Deep L.pdf:/Users/jonasmetzger/Zotero/storage/RZQEKRSH/Sonnet - 2022 - Neuronale Netze kompakt Vom Perceptron zum Deep L.pdf:application/pdf},
}
